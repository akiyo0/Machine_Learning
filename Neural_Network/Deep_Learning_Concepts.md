# ä¼ ç»Ÿç¥ç»ç½‘ç»œ
## æ„æˆè¦ä»¶
### è®­ç»ƒ
Epoch, Batch, Iteration
åŸºæœ¬æ¦‚å¿µï¼š
Epochï¼šä½¿ç”¨è®­ç»ƒé›†çš„å…¨éƒ¨æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œä¸€æ¬¡å®Œæ•´è®­ç»ƒï¼Œè¢«ç§°ä¹‹ä¸ºâ€œä¸€ä»£è®­ç»ƒâ€ï¼›æˆ–ä¸€æ•´æ¬¡å®Œå…¨è®­ç»ƒ
Batchï¼šä½¿ç”¨è®­ç»ƒé›†ä¸­çš„ä¸€å°éƒ¨åˆ†æ ·æœ¬å¯¹æ¨¡å‹æƒé‡è¿›è¡Œä¸€æ¬¡åå‘ä¼ æ’­çš„å‚æ•°æ›´æ–°ï¼Œè¿™ä¸€å°éƒ¨åˆ†æ ·æœ¬è¢«ç§°ä¸ºâ€œä¸€æ‰¹æ•°æ®â€ï¼›
Literationï¼šä½¿ç”¨ä¸€ä¸ª Batch æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°çš„è¿‡ç¨‹ï¼Œè¢«ç§°ä¹‹ä¸ºâ€œä¸€æ¬¡è®­ç»ƒâ€
æ¢ç®—å…³ç³»ï¼š
$$\text{Number of Batches}=\frac{\text { TrainingSet Size }}{\text{Batch Size}}$$
å®é™…ä¸Šï¼Œæ¢¯åº¦ä¸‹é™çš„å‡ ç§æ–¹å¼çš„æ ¹æœ¬åŒºåˆ«å°±åœ¨äºä¸Šé¢å…¬å¼ä¸­çš„ Batch Size ä¸åŒã€‚

| æ¢¯åº¦ä¸‹é™æ–¹å¼ | TrainingSet Size | Batch Size | Number of Batches |
| --- | --- | --- | --- |
| BGD | $N$ | $N$ | $1$ |
| SGD | $N$ | $1$ | $N$ |
| Mini-Batch | $N$ | $B$ | $\frac{N}{B+1}$ |

æ³¨ï¼šä¸Šè¡¨ä¸­ Mini-Batch çš„ Batch ä¸ªæ•°ä¸º $\frac{N}{B+1}$ æ˜¯é’ˆå¯¹æœªæ•´é™¤çš„æƒ…å†µã€‚æ•´é™¤åˆ™æ˜¯ $\frac{N}{B}$ã€‚

e.g. CIFAR10 æ•°æ®é›†æœ‰ 50000 å¼ è®­ç»ƒå›¾ç‰‡ï¼Œ10000 å¼ æµ‹è¯•å›¾ç‰‡ã€‚ç°åœ¨é€‰æ‹© Batch Size = 256 å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚
1. æ¯ä»£ Epoch è¦è®­ç»ƒçš„å›¾ç‰‡æ•°é‡ï¼š$50000$
2. è®­ç»ƒé›†å…·æœ‰çš„ Batch ä¸ªæ•°ï¼š$50000 / 256 \approx 195+1=196$
3. æ¯ä¸ª Epoch éœ€è¦å®Œæˆçš„ Batch ä¸ªæ•°ï¼š$196$
4. æ¯ä¸ª Epoch å…·æœ‰çš„ Iteration ä¸ªæ•°ï¼š$196$
5. æ¯ä¸ª Epoch ä¸­å‘ç”Ÿæ¨¡å‹æƒé‡æ›´æ–°çš„æ¬¡æ•°ï¼š$196$
6. è®­ç»ƒ 10 ä»£åï¼Œæ¨¡å‹æƒé‡æ›´æ–°çš„æ¬¡æ•°ï¼š$196\times10=1960$
7. ä¸åŒä»£çš„è®­ç»ƒï¼Œå…¶å®ç”¨çš„æ˜¯åŒä¸€ä¸ªè®­ç»ƒé›†çš„æ•°æ®ã€‚ç¬¬ 1 ä»£å’Œç¬¬ 10 ä»£è™½ç„¶ç”¨çš„éƒ½æ˜¯è®­ç»ƒé›†çš„å…¨éƒ¨å›¾ç‰‡ï¼Œä½†æ˜¯å¯¹æ¨¡å‹çš„æƒé‡æ›´æ–°å€¼å´æ˜¯å®Œå…¨ä¸åŒçš„ã€‚å› ä¸ºä¸åŒä»£çš„æ¨¡å‹å¤„äºä»£ä»·å‡½æ•°ç©ºé—´ä¸Šçš„ä¸åŒä½ç½®ï¼Œæ¨¡å‹çš„è®­ç»ƒä»£è¶Šé åï¼Œè¶Šæ¥è¿‘è°·åº•ï¼Œå…¶ä»£ä»·è¶Šå°ã€‚
https://www.jianshu.com/p/22c50ded4cf7
### æ¢¯åº¦

**æ¢¯åº¦æ¶ˆå¤±åŠæ¢¯åº¦çˆ†ç‚¸ğŸ’¥é—®é¢˜
(Gradient vanishing and gradient exploding problem)**

**æ¢¯åº¦çˆ†ç‚¸é—®é¢˜**å’Œ**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**ä¸€èˆ¬éšç€ç½‘ç»œå±‚æ•°çš„å¢åŠ ä¼šå˜å¾—è¶Šæ¥è¶Šæ˜æ˜¾ã€‚

ä¾‹å¦‚ï¼Œå¯¹äºä¸‹é¢ä¸€ä¸ªå«æœ‰3ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œï¼ˆ`HDL1`, `HDL2`, `HDL3`ï¼‰
`INT` -> `HDL1` -> `HDL2` -> `HDL3` -> `OUT`
å½“**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**å‘ç”Ÿæ—¶ï¼Œæ¥è¿‘äºè¾“å‡ºå±‚(`OUT`)çš„`HDL3`åŠ`HDL2`çš„æƒå€¼æ›´æ–°ç›¸å¯¹æ­£å¸¸ï¼Œä½†æ¥è¿‘äºè¾“å‡ºå±‚(`INT`)çš„`HDL1`çš„æƒå€¼æ›´æ–°ä¼šå˜å¾—å¾ˆæ…¢ï¼Œ<u>ä»¥è‡³äºè¿™äº›å±‚æƒå€¼æ²¡æœ‰å‘ç”Ÿæ˜¾è‘—æ”¹å˜ï¼Œæ¥è¿‘äºåˆå§‹åŒ–çš„æƒå€¼</u>ï¼Œå¯¼è‡´å‰é¢çš„å±‚åªå¯¹æ‰€æœ‰çš„è¾“å…¥åšäº†ä¸€ä¸ªåŒä¸€æ˜ å°„ï¼ˆå³æ˜ å°„å±‚ï¼‰ã€‚ä¹Ÿè§£é‡Šäº†ä¸ºä½•æ›´æ·±å±‚çš„ç¥ç»ç½‘ç»œçš„å­¦ä¹ ç»“æœä»…ç­‰ä»·äºåå‡ å±‚æµ…å±‚ç½‘ç»œçš„å­¦ä¹ ã€‚
Â 
**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜äº§ç”Ÿçš„å…·ä½“è¿‡ç¨‹**
ä»¥ä¸‹å›¾çš„åå‘ä¼ æ’­ä¸ºä¾‹ï¼ˆå‡è®¾æ¯ä¸€å±‚åªæœ‰ä¸€ä¸ªç¥ç»å…ƒä¸”å¯¹äºæ¯ä¸€å±‚$y_{i}=\sigma\left(z_{i}\right)=\sigma\left(w_{i} x_{i}+b_{i}\right)$ï¼Œå…¶ä¸­ $\sigma$ ä¸ºsigmoidå‡½æ•°ï¼‰
`START` --> `b1` -$w_2$-> `b2` -$w_3$-> `b3` -$w_4$-> `b4` --> `C`
$$\begin{aligned}
\frac{\partial C}{\partial b_{1}} &=\frac{\partial C}{\partial y_{4}} \frac{\partial y_{4}}{\partial z_{4}} \frac{\partial z_{4}}{\partial x_{4}} \frac{\partial x_{4}}{\partial z_{3}} \frac{\partial z_{3}}{\partial x_{3}} \frac{\partial x_{3}}{\partial z_{2}} \frac{\partial z_{2}}{\partial x_{2}} \frac{\partial x_{2}}{\partial z_{1}} \frac{\partial z_{1}}{\partial b_{1}} \\
&=\frac{\partial C}{\partial y_{4}} \sigma^{\prime}\left(z_{4}\right) w_{4} \sigma^{\prime}\left(z_{3}\right) w_{3} \sigma^{\prime}\left(z_{2}\right) w_{2} \sigma^{\prime}\left(z_{1}\right)
\end{aligned}$$

æ ¹æ® $\sigma^{\prime}(x)$ å‡½æ•°åœ¨å®šä¹‰åŸŸ $[-5,5]$ å›¾åƒå¯çŸ¥ã€‚$\sigma^{\prime}(x)$ çš„æœ€å¤§å€¼ä¸º $\frac{1}{4}$ï¼Œè€Œæˆ‘ä»¬åˆå§‹åŒ–çš„ç½‘ç»œæƒå€¼ $|w|$ é€šå¸¸éƒ½å°äº1ï¼Œå› æ­¤ $\left|\sigma^{\prime}(z) w\right| \leq \frac{1}{4}$ï¼Œå› æ­¤å¯¹äºä¸Šé¢çš„é“¾å¼æ±‚å¯¼ï¼Œå±‚æ•°è¶Šå¤šï¼Œæ±‚å¯¼ç»“æœ $\frac{\partial C}{\partial b_{1}}$ è¶Šå°ï¼Œå› è€Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±çš„æƒ…å†µå‡ºç°ã€‚

è¿™æ ·ï¼Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜çš„å‡ºç°åŸå› å°±æ˜¾è€Œæ˜“è§äº†ï¼Œå³ $\left|\sigma^{\prime}(z) w\right|>1$ï¼Œä¹Ÿå°±æ˜¯ $\sigma^{\prime}(z)$ æ¯”è¾ƒå¤§çš„æƒ…å†µã€‚ä½†å¯¹äºä½¿ç”¨sigmoidæ¿€æ´»å‡½æ•°æ¥è¯´ï¼Œè¿™ç§æƒ…å†µæ¯”è¾ƒå°‘ã€‚å› ä¸º $\sigma^{\prime}(z)$ çš„å¤§å°ä¹Ÿä¸ $w$ æœ‰å…³ï¼ˆ$z=wx+b$ï¼‰ï¼Œé™¤éè¯¥å±‚çš„è¾“å…¥å€¼ $x$ åœ¨ä¸€ç›´ä¸€ä¸ªæ¯”è¾ƒå°çš„èŒƒå›´å†…ã€‚

å¦‚æœæ­¤éƒ¨åˆ†å¤§äº1ï¼Œé‚£ä¹ˆå±‚æ•°å¢å¤šçš„æ—¶å€™ï¼Œæœ€ç»ˆçš„æ±‚å‡ºçš„æ¢¯åº¦æ›´æ–°å°†ä»¥æŒ‡æ•°å½¢å¼å¢åŠ ï¼Œå³å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸ï¼Œå¦‚æœæ­¤éƒ¨åˆ†å°äº1ï¼Œé‚£ä¹ˆéšç€å±‚æ•°å¢å¤šï¼Œæ±‚å‡ºçš„æ¢¯åº¦æ›´æ–°ä¿¡æ¯å°†ä¼šä»¥æŒ‡æ•°å½¢å¼è¡°å‡ï¼Œå³å‘ç”Ÿäº†æ¢¯åº¦æ¶ˆå¤±ã€‚
https://blog.csdn.net/qq_25737169/article/details/78847691

å…¶å®æ¢¯åº¦çˆ†ç‚¸å’Œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜éƒ½æ˜¯å› ä¸ºç½‘ç»œå¤ªæ·±ï¼Œç½‘ç»œæƒå€¼æ›´æ–°ä¸ç¨³å®šé€ æˆçš„ï¼Œæœ¬è´¨ä¸Šæ˜¯å› ä¸ºæ¢¯åº¦åå‘ä¼ æ’­ä¸­çš„è¿ä¹˜æ•ˆåº”ã€‚å¯¹äºæ›´æ™®éçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ç”¨ReLUæ¿€æ´»å‡½æ•°å–ä»£sigmoidæ¿€æ´»å‡½æ•°ã€‚å¦å¤–ï¼ŒLSTMçš„ç»“æ„è®¾è®¡ä¹Ÿå¯ä»¥æ”¹å–„RNNä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

https://zhuanlan.zhihu.com/p/25631496




## æŸå¤±å‡½æ•° ç›®æ ‡å‡½æ•° Loss
Cost/Loss(Min) Objective(Max) Functions

### æœ€å¤§ä¼¼ç„¶ä¼°è®¡ [A-a-1]
1. Many cost functions are the result of applying Maximum Likelihood. For instance, the Least Squares cost function can be obtained via Maximum Likelihood. Cross-Entropy is another example.

2. The likelihood of a parameter value (or vector of parameter values), $Î¸$, given outcomes $x$, is equal to the probability (density) assumed for those observed outcomes given those parameter values, that is $$\mathcal{L}(\theta | x)=P(x | \theta)$$

3. The natural logarithm of the likelihood function, called the log-likelihood, is more convenient to work with. Because the logarithm is a monotonically increasing function, the logarithm of a function achieves its maximum value at the same points as the function itself, and hence the log-likelihood can be used in place of the likelihood in maximum likelihood estimation and related techniques.

4. In general, for a ï¬xed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function. Intuitively, this maximizes the "agreement" of the selected model with the observed data, and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution. Maximum-likelihood estimation gives a uniï¬ed approach to estimation, which is well-deï¬ned in the case of the normal distribution and many other problems.
$$f\left(x_{1}, x_{2}, \ldots, x_{n} | \theta\right)=f\left(x_{1} | \theta\right) \times f\left(x_{2} | \theta\right) \times \cdots \times f\left(x_{n} | \theta\right)$$

$$\mathcal{L}\left(\theta ; x_{1}, \ldots, x_{n}\right)=f\left(x_{1}, x_{2}, \ldots, x_{n} | \theta\right)=\prod_{i=1}^{n} f\left(x_{i} | \theta\right)$$

$$\ln \mathcal{L}\left(\theta ; x_{1}, \ldots, x_{n}\right)=\sum_{i=1}^{n} \ln f\left(x_{i} | \theta\right)$$

$$\hat{\ell}(\theta ; x)=\frac{1}{n} \sum_{i=1}^{n} \ln f\left(x_{i} | \theta\right)$$

$$\left\{\hat{\theta}_{\text {mle}}\right\} \subseteq\left\{\underset{\theta \in \Theta}{\arg \max} \hat{\ell}\left(\theta ; x_{1}, \ldots, x_{n}\right)\right\}$$

### äº¤å‰ç†µ (Cross-Entropy) [A-a-2]
Cross entropy can be used to deï¬ne the loss function in machine learning and optimization. The true probability $p_i$ is the true label, and the given distribution $q_i$ is the predicted value of the current model.
$$\begin{aligned}
&H(p, q)=\mathrm{E}_{p}\left[l_{i}\right]=\mathrm{E}_{p}\left[\log \frac{1}{q\left(x_{i}\right)}\right]\\
&H(p, q)=\sum_{x_{i}} p\left(x_{i}\right) \log \frac{1}{q\left(x_{i}\right)}\\
&H(p, q)=-\sum_{x} p(x) \log q(x)
\end{aligned}
$$

Cross-entropy error function and logistic regression
$$
L(\mathbf{w})=\frac{1}{N} \sum_{n=1}^{N} H\left(p_{n}, q_{n}\right)=-\frac{1}{N} \sum_{n=1}^{N}\left[y_{n} \log \hat{y}_{n}+\left(1-y_{n}\right) \log \left(1-\hat{y}_{n}\right)\right]
$$


## æœ€ä¼˜åŒ–
### ä¼˜åŒ–å™¨
#### åŠ¨é‡ï¼ˆMomentumï¼‰
mini-batch SGD ç®—æ³•è™½ç„¶æœ‰å¾ˆå¿«çš„è®­ç»ƒé€Ÿåº¦ï¼Œä½†ç»“æœå¹¶ä¸æ€»æ˜¯å…¨å±€æœ€ä¼˜ã€‚å¦å¤–éœ€è¦æŒ‘é€‰åˆé€‚çš„è¶…å‚ï¼ˆå­¦ä¹ ç‡ï¼‰ï¼Œä¸åˆé€‚çš„è¶…å‚ä¼šå¯¼è‡´æ”¶æ•›é€Ÿåº¦è¿‡æ…¢ï¼ˆéœ‡è¡æ”¶æ•›ï¼Ÿï¼‰æˆ–è·³è¿‡æœ€ä¼˜åŒºé—´ã€‚
MomentumåŸºäºæ¢¯åº¦çš„ç§»åŠ¨æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œå¯ä»¥è§£å†³mini-batch SGDä¼˜åŒ–ç®—æ³•æ›´æ–°å¹…åº¦æ‘†åŠ¨å¤§çš„é—®é¢˜ï¼ŒåŒæ—¶å¯ä»¥ä½¿å¾—ç½‘ç»œçš„æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚
å‡è®¾åœ¨å½“å‰çš„è¿­ä»£æ­¥éª¤ç¬¬ $t$ æ­¥ä¸­ï¼Œé‚£ä¹ˆåŸºäºMomentumä¼˜åŒ–ç®—æ³•å¯ä»¥å†™æˆä¸‹é¢çš„å…¬å¼ï¼š 
$$\begin{array}{c}
v_{d w}=\beta v_{d w}+(1-\beta) d W \\
v_{d b}=\beta v_{d b}+(1-\beta) d b \\
W=W-\alpha v_{d w} \\
b=b-\alpha v_{d b}
\end{array}$$
å…¶ä¸­ï¼Œåœ¨ä¸Šé¢çš„å…¬å¼ä¸­ $v_{dw}$ å’Œ $v_{db}$ åˆ†åˆ«æ˜¯æŸå¤±å‡½æ•°åœ¨å‰ $tâˆ’1$ è½®è¿­ä»£è¿‡ç¨‹ä¸­ç´¯ç§¯çš„æ¢¯åº¦åŠ¨é‡ï¼Œ$Î²$ æ˜¯æ¢¯åº¦ç´¯ç§¯çš„ä¸€ä¸ªæŒ‡æ•°ï¼Œä¸€èˆ¬è®¾ç½®å€¼ä¸º $Î²=0.9$ã€‚$dW$ å’Œ $db$ åˆ†åˆ«æ˜¯æŸå¤±å‡½æ•°åå‘ä¼ æ’­æ—¶å€™æ‰€æ±‚å¾—çš„æ¢¯åº¦ï¼Œä¸‹é¢ä¸¤ä¸ªå…¬å¼æ˜¯ç½‘ç»œæƒé‡å‘é‡å’Œåç½®å‘é‡çš„æ›´æ–°å…¬å¼ï¼Œ$Î±$ æ˜¯ç½‘ç»œçš„å­¦ä¹ ç‡ã€‚
æ‰€ä»¥Momentumä¼˜åŒ–å™¨çš„ä¸»è¦æ€æƒ³å°±æ˜¯åˆ©ç”¨äº†ç±»ä¼¼ä¸**æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡æ³•[^EWMA]**çš„æ–¹æ³•æ¥å¯¹ç½‘ç»œçš„å‚æ•°è¿›è¡Œå¹³æ»‘å¤„ç†çš„ï¼Œè®©æ¢¯åº¦çš„æ‘†åŠ¨å¹…åº¦å˜å¾—æ›´å°ã€‚ 
* [ ] ???ä¸ºä»€ä¹ˆ

[^EWMA]: **æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡æ³•(EWMA)**ï¼šExponentially weighted averagesã€‚æ ¹æ®åŒä¸€ä¸ªç§»åŠ¨æ®µå†…ä¸åŒæ—¶é—´çš„æ•°æ®å¯¹é¢„æµ‹å€¼çš„å½±å“ç¨‹åº¦ï¼Œåˆ†åˆ«ç»™äºˆä¸åŒçš„æƒæ•°ï¼Œç„¶åå†è¿›è¡Œå¹³å‡ç§»åŠ¨ä»¥é¢„æµ‹æœªæ¥å€¼ã€‚åŠ æƒç§»åŠ¨å¹³å‡æ³•ï¼Œæ˜¯å¯¹è§‚å¯Ÿå€¼åˆ†åˆ«ç»™äºˆä¸åŒçš„æƒæ•°ï¼ŒæŒ‰ä¸åŒæƒæ•°æ±‚å¾—ç§»åŠ¨å¹³å‡å€¼ï¼Œå¹¶ä»¥æœ€åçš„ç§»åŠ¨å¹³å‡å€¼ä¸ºåŸºç¡€ï¼Œç¡®å®šé¢„æµ‹å€¼çš„æ–¹æ³•ã€‚é‡‡ç”¨åŠ æƒç§»åŠ¨å¹³å‡æ³•ï¼Œæ˜¯å› ä¸ºè§‚å¯ŸæœŸçš„è¿‘æœŸè§‚å¯Ÿå€¼å¯¹é¢„æµ‹å€¼æœ‰è¾ƒå¤§å½±å“ï¼Œå®ƒæ›´èƒ½åæ˜ è¿‘æœŸå˜åŒ–çš„è¶‹åŠ¿ã€‚æŒ‡æ•°ç§»åŠ¨åŠ æƒå¹³å‡æ³•ï¼Œæ˜¯æŒ‡å„æ•°å€¼çš„åŠ æƒç³»æ•°éšæ—¶é—´å‘ˆæŒ‡æ•°å¼é€’å‡ï¼Œè¶Šé è¿‘å½“å‰æ—¶åˆ»çš„æ•°å€¼åŠ æƒç³»æ•°å°±è¶Šå¤§ã€‚æŒ‡æ•°ç§»åŠ¨åŠ æƒå¹³å‡è¾ƒä¼ ç»Ÿçš„å¹³å‡æ³•æ¥è¯´ï¼Œä¸€æ˜¯ä¸éœ€è¦ä¿å­˜è¿‡å»æ‰€æœ‰çš„æ•°å€¼ï¼›äºŒæ˜¯è®¡ç®—é‡æ˜¾è‘—å‡å°ã€‚

#### RMSprop
åœ¨ä¸Šé¢çš„Momentumä¼˜åŒ–ç®—æ³•ä¸­ï¼Œè™½ç„¶åˆæ­¥è§£å†³äº†æŸå¤±å‡½æ•°åœ¨æ›´æ–°ä¸­å­˜åœ¨çš„æ‘†åŠ¨å¹…åº¦[^æ‘†åŠ¨å¹…åº¦]è¿‡å¤§é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥åŠ å¿«å‡½æ•°çš„æ”¶æ•›é€Ÿåº¦ï¼ŒGeoffrey E. Hintonåœ¨Courseraè¯¾ç¨‹ä¸­æå‡ºçš„ä¸€ç§æ–°çš„ä¼˜åŒ–ç®—æ³•ã€‚
[^æ‘†åŠ¨å¹…åº¦]: æ‘†åŠ¨å¹…åº¦ï¼šæŒ‡åœ¨ä¼˜åŒ–ä¸­ç»è¿‡æ›´æ–°ä¹‹åå‚æ•°çš„å˜åŒ–èŒƒå›´ã€‚
RMSPropç®—æ³•çš„å…¨ç§°å« Root Mean Square Propï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒRMSPropç®—æ³•æ‹¥æœ‰è¾ƒMomentumç®—æ³•æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚
![](https://img-blog.csdn.net/20170923134334368?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGR1YW4x/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
è“è‰²çš„ä¸ºMomentumä¼˜åŒ–ç®—æ³•æ”¶æ•›è·¯å¾„ï¼Œç»¿è‰²çš„ä¸ºRMSPropä¼˜åŒ–ç®—æ³•æ”¶æ•›è·¯å¾„ã€‚ 

ä¸Momentumç®—æ³•ä¸åŒçš„æ˜¯ï¼ŒRMSPropç®—æ³•å¯¹æ¢¯åº¦è®¡ç®—äº†**å¾®åˆ†å¹³æ–¹åŠ æƒå¹³å‡æ•°**ã€‚è¿™æ ·åˆ©äºæ¶ˆé™¤æ‘†åŠ¨å¹…åº¦å¤§çš„æ–¹å‘ï¼Œä¿®æ­£æ‘†åŠ¨å¹…åº¦ï¼Œä½¿å¾—å„ä¸ªç»´åº¦çš„æ‘†åŠ¨å¹…åº¦é™ä½ã€‚å¦ä¸€æ–¹é¢ä¹Ÿä½¿å¾—ç½‘ç»œå‡½æ•°æ”¶æ•›æ›´å¿«ã€‚

RMSPropç®—æ³•å¯¹æƒé‡ $W$ å’Œåç½® $b$ çš„æ¢¯åº¦ä½¿ç”¨äº†**å¾®åˆ†å¹³æ–¹åŠ æƒå¹³å‡æ•°**ã€‚ å…¶ä¸­ï¼Œå‡è®¾åœ¨ç¬¬ $t$ è½®è¿­ä»£è¿‡ç¨‹ä¸­ï¼š 
$$\begin{aligned}
s_{d w} &=\beta s_{d w}+(1-\beta) d W^{2} \\
s_{d b} &=\beta s_{d b}+(1-\beta) d b^{2} \\
W &=W-\alpha \frac{d W}{\sqrt{s_{d b}+\varepsilon}} \\
b &=b-\alpha \frac{d b}{\sqrt{s_{d b}+\varepsilon}}
\end{aligned}$$

åœ¨ä¸Šé¢çš„å…¬å¼ä¸­ $s_{dw}$ å’Œ $s_{db}$ åˆ†åˆ«æ˜¯æŸå¤±å‡½æ•°åœ¨å‰ $tâˆ’1$ è½®è¿­ä»£è¿‡ç¨‹ä¸­ç´¯ç§¯çš„æ¢¯åº¦æ¢¯åº¦åŠ¨é‡ï¼Œ$Î²$ æ˜¯æ¢¯åº¦ç´¯ç§¯çš„ä¸€ä¸ªæŒ‡æ•°ã€‚ï¼ˆå½“ $dW$ æˆ–è€… $db$ ä¸­æœ‰ä¸€ä¸ªå€¼æ¯”è¾ƒå¤§çš„æ—¶å€™ï¼Œé‚£ä¹ˆåœ¨æ›´æ–°æƒé‡æˆ–è€…åç½®çš„æ—¶å€™é™¤ä»¥å®ƒä¹‹å‰ç´¯ç§¯çš„æ¢¯åº¦çš„å¹³æ–¹æ ¹ï¼Œè¿™æ ·å°±å¯ä»¥ä½¿å¾—æ›´æ–°å¹…åº¦å˜å°ï¼‰ã€‚ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸ºé›¶ï¼Œä½¿ç”¨äº†ä¸€ä¸ªå¾ˆå°çš„æ•°å€¼ $Ïµ$ æ¥è¿›è¡Œå¹³æ»‘ï¼Œä¸€èˆ¬å–å€¼ä¸º $10^{âˆ’8}$ã€‚

#### Adam
ä»¥ä¸Šä¸¤ç§ä¼˜åŒ–ç®—æ³•ï¼Œä¸€ç§æ˜¯ç±»ä¼¼äºç‰©ç†ä¸­çš„åŠ¨é‡æ¥ç´¯ç§¯æ¢¯åº¦ï¼Œå¦ä¸€ç§ä½¿å¾—æ”¶æ•›é€Ÿåº¦æ›´å¿«åŒæ—¶å‡å°æŸå¤±å‡½æ•°çš„æ³¢åŠ¨ã€‚Adamï¼ˆAdaptive Moment Estimationï¼‰ç®—æ³•æ˜¯å°†Momentumç®—æ³•å’ŒRMSPropç®—æ³•ç»“åˆèµ·æ¥çš„ä¸€ç§ç®—æ³•ã€‚
ä½¿ç”¨çš„å‚æ•°åŸºæœ¬å’Œç»“åˆä¹‹å‰ä¿æŒä¸€è‡´ï¼Œåœ¨è®­ç»ƒçš„å¼€å§‹å‰éœ€è¦åˆå§‹åŒ–æ¢¯åº¦ç´¯ç§¯é‡å’Œå¹³æ–¹ç´¯ç§¯é‡ã€‚ 
$$v_{d w}=0, v_{d b}=0 ; s_{d w}=0, s_{d b}=0$$


