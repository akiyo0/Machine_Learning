# 章节1: 深度学习初见
课时1: 深度学习框架简介
课时2: PyTorch功能演示
# 章节2: 开发环境安装试看
课时3: Anaconda-CUDA安装
课时4: PyTorch-PyCharm安装
# 章节3: 回归问题试看
课时5: 简单回归问题
课时6: 简单回归问题
课时7: 回归问题实战
课时8: 分类问题引入
课时9: 分类问题引入
课时10: 手写数字识别初体验
课时11: 手写数字识别初体验
课时12: 手写数字识别初体验
课时13: 手写数字识别初体验
课时14: 手写数字识别初体验
# 章节4: PyTorch基础教程
课时15: 张量数据类型
课时16: 张量数据类型
课时17: 创建Tensor
课时18: 创建Tensor
课时19: 索引与切片
课时20: 索引与切片
课时21: 维度变换
课时22: 维度变换
课时23: 维度变换
课时24: 维度变换

# 章节5: PyTorch进阶教程
课时25: Broadcasting
课时26: Broadcasting
课时27: Broadcasting
课时28: 合并与分割
课时29: 合并与分割
课时30: 数学运算
课时31: 数学运算
课时32: 属性统计
课时33: 属性统计
课时34: 高阶操作
# 章节6: 随机梯度下降
课时35: 什么是梯度
课时36: 什么是梯度
课时37: 常见函数的梯度
课时38: 激活函数与Loss的梯度
课时39: 激活函数与Loss的梯度
课时40: 激活函数与Loss的梯度
课时41: 激活函数与Loss的梯度
课时42: 感知机的梯度推导
课时43: 感知机的梯度推导
课时44: 链式法则
课时45: 反向传播算法
课时46: 反向传播算法
课时47: 优化问题实战
# 章节7: 神经网络与全连接层
课时48: Logistic Regression
课时49: 交叉熵
课时50: 交叉熵
课时51: 交叉熵
课时52: 多分类问题实战
课时53: 全连接层
课时54: 激活函数与GPU加速
课时55: MNIST测试实战
课时56: Visdom可视化

# 章节8: 过拟合
课时57: 过拟合与欠拟合
课时58: 交叉验证
课时59: 交叉验证
课时60: Regularization
课时61: 动量与学习率衰减
课时62: Early stopping, dropout等
# 章节9: 卷积神经网络CNN
课时63: 什么是卷积
课时64: 什么是卷积
课时65: 卷积神经网络
课时66: 卷积神经网络
课时67: 卷积神经网络
课时68: 池化层与采样
课时69: BatchNorm
课时70: BatchNorm
课时71: BatchNorm
课时72: 经典卷积网络 LeNet5,AlexNet, VGG, GoogLeNet
课时73: 经典卷积网络 LeNet5,AlexNet, VGG, GoogLeNet
课时74: ResNet与DenseNet
课时75: ResNet与DenseNet
课时76: nn.Module模块
课时77: nn.Module模块
课时78: 数据增强
# 章节10: CIFAR10与ResNet实战
课时79: CIFAR10数据集介绍
课时80: 卷积神经网络实战
课时81: 卷积神经网络实战
课时82: 卷积神经网络训练
课时83: ResNet实战
课时84: ResNet实战
课时85: ResNet实战
课时86: ResNet实战
课时87: 实战小结
# 章节11: 循环神经网络RNN&LSTM
课时88: 时间序列表示方法
课时89: RNN原理
课时90: RNN原理
课时91: RNN Layer使用
课时92: RNN Layer使用
课时93: 时间序列预测实战
课时94: 梯度弥散与梯度爆炸
课时95: LSTM原理
课时96: LSTM原理
课时97: LSTM Layer使用
课时98: 情感分类问题实战
# 章节12: 迁移学习-实战宝可梦精灵
课时99: Pokemon数据集
课时100: 数据预处理
课时101: 自定义数据集实战
课时102: 自定义数据集实战
课时103: 自定义数据集实战
课时104: 自定义数据集实战
课时105: 自定义数据集实战
课时106: 自定义网络
课时107: 自定义网络训练与测试
课时108: 自定义网络实战
课时109: 迁移学习
课时110: 迁移学习实战
# 章节13: 自编码器Auto-Encoders
课时111: 无监督学习
课时112: Auto-Encoder原理
课时113: Auto-Encoder变种
课时114: Adversarial Auto-Encoder
课时115: 变分Auto-Encoder引入
课时116: Reparameterization trick
课时117: 变分自编码器VAE
课时118: Auto-Encoder实战
课时119: Auto-Encoder实战
课时120: 变分Auto-Encoder实战
课时121: 变分Auto-Encoder实战
# 章节14: 对抗生成网络GAN
课时122: 数据的分布
课时123: 画家的成长历程
课时124: GAN原理
课时125: 纳什均衡-D
课时126: 纳什均衡-G
课时127: JS散度的缺陷
课时128: EM距离
课时129: WGAN与WGAN-GP
课时130: GAN实战-GD实现
课时131: GAN实战-网络训练
课时132: GAN实战-网络训练鲁棒性
课时133: WGAN-GP实战
# 章节15: 选看：Ubuntu开发环境安装
课时134: Ubuntu系统安装
课时135: Anaconda安装
课时136: CUDA 10安装
课时137: 环境变量配置
课时138: cudnn安装
课时139: PyCharm安装与配置
# 章节16: 选看：人工智能发展简史
课时140: 生物神经元结构
课时141: 感知机的提出
课时142: BP神经网络
课时143: CNN和LSTM的发明
课时144: 人工智能的低潮
课时145: 深度学习的诞生
课时146: 深度学习的繁荣
# 章节17: 选看：Numpy实战BP神经网络
课时147: 权值的表示
课时148: 多层感知机的实现
课时149: 多层感知机前向传播
课时150: 多层感知机反向传播
课时151: 多层感知机反向传播
课时152: 多层感知机反向传播
课时153: 多层感知机的训练
课时154: 多层感知机的测试
课时155: 实战小结



<b>课时1 深度学习框架简介</b>
PyTorch和TensorFlow 本质区别：静态图框架/动态图框架
TensorFlow 使用静态图，先定义计算图后使用。PyTorch 中，每次都会重新构建一个新的计算图。

课时2 PyTorch功能演示
1. GPU加速

2. 自动求导
$$y=a^2 x+bx+c\text{  WHEN  } x=1$$

$\frac{\partial y}{\partial a}=2ax,\frac{\partial y}{\partial b}=x,\frac{\partial y}{\partial c}=1$

```python
grads = autograd.grad(y, [a, b, c])
```

let $x=1$，$a=1, b=2, c=3$ then grads is `2, 1, 1`

3. 各种常用网络层
nn.Linear, nn.Conv2d, nn.LSTM, nn.ReLU, nn.Sigmoid, nn.Softmax, nn.CrossEntrpyLoss, nn.MSE etc.

课时3 PyTorch部署
课时4 简单回归问题
Closed Form Solution 闭式解

**课时6 回归问题实战**

**课时7 分类问题引入-1**

**课时18 课时19 索引与切片**

索引

```python
torch.rand(4, 3, 28, 28)[0] # ==> torch.Size([3, 28, 28])
torch.rand(4, 3, 28, 28)[0, 0] #==> torch.Size([28, 28])
torch.rand(4, 3, 28, 28)[0, 0, 0, 0] #==> tensor(0.8082)
```

连续索引

```python
torch.rand(4, 3, 28, 28)[:2].shape # torch.Size([2, 3, 28, 28])
torch.rand(4, 3, 28, 28)[:2, -1:, :, :].shape # torch.Size([2, 1, 28, 28])
torch.rand(4, 3, 28, 28)[:, :, 0:28:2, 0:28:2].shape # torch.Size([4, 3, 14, 14])
```

-1 反向索引

定向索引

```python
torch.Size([4, 3, 28, 28]).index_select(0, torch.arange(8)).shape 
#torch.Size([2, 3, 28, 28])
```

剩余元素索引

```python
torch.Size([4, 3, 28, 28])[...].shape #torch.Size([4, 3, 28, 28])
torch.Size([4, 3, 28, 28])[...,:2].shape #torch.Size([4, 3, 28, 2])
torch.Size([4, 3, 28, 28])[:, 1, ...] == torch.Size([4, 3, 28, 28])[:, 1] 
```

掩码索引

```python
torch.randn(3, 4).ge(0.5) #标识大于（g）等于（e）0.5的元素为1
```

```
tensor([[-0.3807,  0.7432, -2.1667, -1.1932],
        [ 0.5556, -1.4348, -0.5310,  0.2316],
        [ 0.9051,  0.9219, -0.5962, -0.2291]])
tensor([[False, False,  True, False],
        [False, False,  True, False],
        [ True, False,  True, False]])
```

```python
torch.masked_select(x, x.ge(0.5)) #选出标识为1的元素，注意输出的shape发生改变
```

```
tensor([1.7225, 1.4088, 0.5096])
```



**课时34 课时35 什么是梯度**
导数 (Derivate) 标量
偏导数/微分 (Partial Derivate)
梯度 (Gradient) 向量

$\nabla f=\left(\frac{\partial f}{\partial x_{1}} ; \frac{\partial f}{\partial x_{2}} ; \ldots ; \frac{\partial f}{\partial x_{n}}\right)$

How to search for minima?
梯度下降法

鞍点(Saddle Point)

Optimizer Performance
(1) initialization status; (2) learning rate; (3) momentum(下降惯性); etc. 

**课时36 常见函数梯度**

![image-20201212212439005](/Users/caujoeng/Library/Application Support/typora-user-images/image-20201212212439005.png)

**课时51 全连接层**
两种风格的API：(1) Class-style API; (2) Function-style API;

 ```python
torch.nn.ReLU(inplace: bool = False) #(1)
torch.nn.functional.relu(input, inplace=False) #(2)
 ```



**课时52 激活函数与GPU加速**

CPU accelerated

```python
device = torch.device('cuda:0')
net.Net().to(device) #网络结构可搬
criteon = nn.CrossEntropyLoss().to(device) #损失函数可搬

for epoch in range(epochs):
  for batch_idx, (data, target) in enumerate(train_loader):
    data = data.view(-1, 28*28)
		data, target = data.to(device), target.to(device)
```





[预] 归一化（Normalization）将不定值映射到 \[a, b\](通常为[0, 1]) 的区间之中

+ min-max标准化 (Min-max Normalization)

  $$x^{*}=\frac{x-\min }{\max -\min }$$

标准化（Standardization）将数据映射到均值0，方差1的特定分布中。

+ Z-Score 标准化（Zero-mean normalization）

  $$x^{*}=\frac{x-\mu}{\sigma}$$

将像素值大小不同的照片映射为相同尺度，转换为具有相似特征分布的问题。
一定程度上消除了因为过度曝光，质量不佳或者噪声等各种原因对模型权值更新的影响。[来源1]
如果训练前期，某一RGB通道分量过大，会导致特征值被某一特征所“主导”，权值更新会受“主导”的特征所影响。

[来源1]: https://zhuanlan.zhihu.com/p/35597976
[权威来源2]: https://cs231n.github.io/neural-networks-2/#datapre



**课时 53 MNIST测试实战**

```torch.argmax(input)``` Returns the indices of the maximum value of all elements in the `input` tensor.
If there are multiple maximal values then the indices of the first maximal value are returned.

```python
a = torch.randn(4, 4)
torch.argmax(a)
torch.argmax(a, dim=1)
```

```
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
tensor(0)
tensor([ 0,  2,  0,  1])
```

test once <u>per several batch</u> 
test once <u>per epoch</u> 



**课时54  Visdom可视化**

```shell
pip3 install tensorboard
pip3 install visdom
python3 -m visdom.server
```

**课时55 过拟合与欠拟合**

**课时56 交叉验证**





**课时67 Batch Norm**

Intuitive explanation(直观解释)：为了使梯度下降时，使loss函数的等高线均匀分布，最优解的寻优过程明显会变得平缓，使其更快收敛。Image Normalization：
Batch Normalizatinon：

