
### 9.1 图像增广
前提：需要通过⼤规模数据集使深度神经⽹络成功应⽤
图像增⼴（image augmentation）：为了扩⼤训练数据集的规模，通过对训练图像的⼀系列随机改变，产⽣相似但⼜不同的训练样本。
另一种解释：通过随机改变训练样本，使模型降低对某些属性的依赖。提⾼林模型的泛化能⼒（generalization ability）。
手段包括：剪裁、移动。调整亮度、色彩等。

### 9.2 迁移学习（transfer learning）
通过模型对数据集进行训练，抽取[边缘、纹理、形状和物体组成等]通⽤的图像特征进行分析，提高模型精度，减少对单一种类的过拟合，降低数据采集成本。
迁移学习中的常用技术：微调（fine tuning）。
1. 在源数据集（如ImageNet数据集）上预训练⼀个神经⽹络模型，即源模型。
2. 创建⼀个新的神经⽹络模型，即⽬标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适⽤于⽬标数据集。我们还假设源模型的输出层与源数据集的标签紧密相关，因此在⽬标模 型中不予采⽤。
3. 为⽬标模型添加⼀个输出⼤小为⽬标数据集类别个数的输出层，并随机初始化该层的模型参数。
4. 在⽬标数据集（如椅⼦数据集）上训练⽬标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。

### 9.3-1 选择性搜索[^ss]算法
图像分割是计算机视觉领域很重要的一项任务。选择性搜索算法，应用于图像分割领域。也是目标识别领域的基础。

图像分割、目标识别选择以深度学习来实现的一部分原因：这样可以省去人工特征的构造和分类算法的设计，提升泛化能力。

ss算法主要用于解决目标识别领域中，输入分类器候选区域的选择问题。
ss算法并不是表达每个物体的精确位置，而是提供多个候选区域。在候选区域中有很小一部分会和我们想要的窗口大小和位置很接近。后续再通过其他手段[^其他手段] 来去除冗余的一些窗口。

参考原文《Efficient Graph-Based Image Segmentation》

基本思想：先对图像进行分割(尽量打碎)，[使用基于图论的图像分割方法]分成尽可能多的不重复的子区域（$A=R_{1} \cup R_{2} \cup \ldots \cup R_{n}$）。接下来算法根据子区域的大小,尺寸,颜色或纹理等方法计算相邻子区域的相似度，依次往复直到求出所有的相似度 $s_{ij}$ ，构成集合 $S$。

随后需找出 $S$ 集合中值最大的元素[相似度最高的两个子区域]。
$$ i, j=\arg \max_{ij} \left(S_{i, j}\right)$$

若 $\max(S) = S_{1,2}$ 则
1. 合并子区域 $R_1$，$R_2$ 为新的子区域 $R_{n+1}$，并入集合 $A$ 中。$A=A \cup R_{n+1}$
2. 在集合 $S$ 中删除合并前与**子区域**所有相关的相似度。$S=S/ S_{1, *}, S=S/S_{2, *}$
3. 重复以上操作，直至 $S$ 为空集。

$$\begin{array}{c}
S=\emptyset \\
A=R_{1} \cup R_{2} \cup \ldots \cup R_{n+1} \cup \ldots \cup R_{n+m}
\end{array}$$

此时 $A$ 集合中多出了 $n+m$ 个元素，这些元素是由原先不同的相邻的子区域组合而成的。这个算法给出的候选区域的数量是可以通过参数进行调节的。

我们将这些候选区域称为 Region Proposals。

[^ss]: Selective Search
[^其他手段]: 非极大值抑制

### 9.3 ⽬标检测
图像分类任务中，只有一个主体目标图像里，主要关注该目标的类别。
目标检测(object detection)或物体检测则是针对图像中多个感兴趣的目标，分辨目标类别(事先制指定好的)和图中的具体位置。
在无人驾驶、安防、医疗等领域中有着普遍应用。
目标检测有很长的历史：(本例只展示基于深度学习的目标识别算法)：
2014(R-CNN, SPP-Net) -> 2015(Fast R-CNN, Faster R-CNN) -> 2016(SSD, YOLO)
以下会列出⽬标检测问题⾥的多个深度学习模型。

#### 9.3.1 边界框
目标检测问题，通常使用[矩形的]边界框(bounding box)标定目标位置。
边界框可以由矩形左上角的坐标 $(x_1,y_1)$ 及其右下角的坐标 $(x_4, y_4)$ 确定。

### 9.4 锚框
以每个像素为中⼼⽣成多个⼤小和宽⾼⽐（aspect ratio）不同的边界框。这些边界框被称为**锚框**（anchor box）。
#### 9.4.1 ⽣成多个锚框
需要简化。。。。。。
假设输入图像高为$h$，宽为$w$。我们分别以图像的每个像素为中心生成不同形状的锚框。设大小为$s\in (0,1]$且宽高比为$r > 0$，那么锚框的宽和高将分别为$ws\sqrt{r}$和$hs/\sqrt{r}$。当中心位置给定时，已知宽和高的锚框是确定的。

下面我们分别设定好一组大小$s_1,\ldots,s_n$和一组宽高比$r_1,\ldots,r_m$。如果以每个像素为中心时使用所有的大小与宽高比的组合，输入图像将一共得到$whnm$个锚框。虽然这些锚框可能覆盖了所有的真实边界框，但计算复杂度容易过高。因此，我们通常只对包含$s_1$或$r_1$的大小与宽高比的组合感兴趣，即

$$(s_1, r_1), (s_1, r_2), \ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \ldots, (s_n, r_1).$$

也就是说，以相同像素为中心的锚框的数量为$n+m-1$。对于整个输入图像，我们将一共生成$wh(n+m-1)$个锚框。

以上生成锚框的方法实现在下面的`MultiBoxPrior`函数中。指定输入、一组大小和一组宽高比，该函数将返回输入的所有锚框。
#### 9.4.2 交并⽐
Jaccard系数[^IoU]：⼆者交集⼤小除以⼆者并集
$$J(\mathcal{A}, \mathcal{B})=\frac{|\mathcal{A} \cap \mathcal{B}|}{|\mathcal{A} \cup \mathcal{B}|}$$
[^IoU]: 当衡量两个边界框的相似度时，通常称为交并⽐（intersection over union，IoU）
#### 9.4.3 标注训练集的锚框
#### 9.4.4 输出预测边界框

### 9.5 多尺度目标检测
### 9.6 目标检测数据集
e.g. 皮卡丘

### 9.8 R-CNN
R-CNN 区域卷积神经⽹络（Region-based CNN or Regions with CNN features）是将深度模型应用于目标检测的开创性工作之一。
+ R-CNN：对图像选取若干提议区域，然后用卷积神经网络对每个提议区域做前向计算抽取特征，再用这些特征预测提议区域的类别和边界框。
+ 快速的R-CNN（Fast R-CNN）对R-CNN的一个主要改进在于只对整个图像做卷积神经网络的前向计算。它引入了兴趣区域池化层，从而令兴趣区域能够抽取出形状相同的特征。
+ 更快的R-CNN（Faster R-CNN）将Fast R-CNN中的选择性搜索替换成区域提议网络，从而减少提议区域的生成数量，并保证目标检测的精度。
+ 掩码R-CNN（Mask R-CNN）在Faster R-CNN基础上引入一个全卷积网络，从而借助目标的像素级位置进一步提升目标检测的精度。

#### 9.8.1 R-CNN[^R-CNN]
1. 获得 `INPUT IMAGE` 
2. 将其拆解为两千张候选的区域（Extract region proposals）使用 `selective search` 算法
3. 将每一张 region proposal 输入一个神经网路 (e.g. CNN)
4. 全连接层会输出[^OUTPUT]对应的特征向量，将其输入到多个线性二分类器(e.g. SVM)中，来判断类别。
5. 使用回归网络修正 region proposal 位置。
6. 通过非极大值抑制[^NMS]得到最终很精确的每一个物体的位置。

权重初始化问题
（目的：使参数更快收敛到极小值点）使用 微调(Fine Tune) 

R-CNN与简单图像分类一样需要很多训练样本，区别为前者中每一个训练样本都需要进行标注处理。标注处理过的 bounding box 称作 基准真相（Ground Truth），或者理解为基准 bounding box。

训练目的：对于新的输入，R-CNN 可以将图中的每一个物体以 Ground Truth 的精度标注出来。

对于未收敛于GT的 region proposal 在分类时，应计算IOU：
$$\text {label}=\left\{\begin{array}{cl}
\text {dog} & \text {IOU}>50 \% \\
\text {background} & \text {IOU} \leq 50 \%
\end{array}\right.$$

设 region proposal 位置：$P=\left(P_{x}, P_{y}, P_{w}, P_{h}\right)$
Ground Truth 的位置：$G=\left(G_{x}, G_{y}, G_{w}, G_{h}\right)$

回归网络的训练目标是学习一个 P->G 的映射

$$\begin{aligned}
&\hat{G}_{x}=P_{w} d_{x}(P)+P_{x}\\
&\hat{G}_{y}=P_{h} d_{y}(P)+P_{y}\\
&\hat{G}_{w}=P_{w} e^{d_{w}(P)}\\
&\hat{G}_{h}=P_{w} e^{d_{h}(P)}
\end{aligned}$$

$$d_{*}(P)=w_{*}^{T} \phi_{5}(P)$$

其中 $d_{x}(P)$, $d_{y}(P)$, $d_{w}(P)$, $d_{h}(P)$ 是四个线性函数，输入为 $P$（即NN第五层输出的特征向量），输出为一个实数。
训练本质是求解一个最优化问题。
$$w_{*}=\arg \min _{\hat{w}_{*}} \sum_{i}^{N}\left(t_{*}^{i}-\hat{w}_{*}^{T} \phi_{5}\left(P^{i}\right)\right)^{2}+\lambda\left\|\hat{w}_{*}\right\|^{2}$$
求出四个 $w$ 向量，使得预测值 $G$ 和真实值 $G$ 相差最小。用差平方之和代表距离。同时为了抑制过拟合增加了正则项。
$$\begin{aligned}
&t_{x}=\frac{\left(G_{x}-P_{x}\right)}{P_{w}}\\
&t_{y}=\frac{\left(G_{y}-P_{y}\right)}{P_{h}}\\
&t_{w}=\log \left(\frac{G_{w}}{P_{w}}\right)\\
&t_{h}=\log \left(\frac{G_{h}}{P_{h}}\right)
\end{aligned}$$
训练完成后就会得到对应的四个映射关系，测试时这四个映射关系就能够对预测的 Region Proposal 进行相应的位置调整。提升 bounding box 位置精确度。

最后通过位置回归修正 bounding box 的位置之后，对于同一个物体会得到多个 bounding box。对这些 bounding box 本别打分，通过非极大值抑制去除冗余 bounding box。

非极大值抑制可以理解为：在相邻的 bounding box 中去除分数不是极大值的 bounding box。这里的分数也可叫做置信度。也就网络对于自己预测的结果到底有多少信心。

SVM正负样本的定义和神经网络正负样本定义不是很一样。
正样本是 Ground Truth；负样本是和 Ground Truth 重合率小于 30% 的 Region Proposal。
使用SVM来代替CNN做分类的原因:CNN本身挑选出来的正样本中存在很多位置不精确的 Bounding Box。导致最后物体识别的 Bounding Box 位置也不是很准确。

原论文同时使用了 Hard Mining。可以简单理解为将负样本中那些容易和正样本混淆的样本单独拿出来训练。

引用来源：
https://handong1587.github.io/study/2017/11/28/courses.html

[^R-CNN]: 引用来源：2020海华AI挑战赛·垃圾识别 目标识别与R-CNN概述
[^OUTPUT]: 输出层为指定的目标类别，另包括背景类。
[^NMS]: NMS(Non Maximum Suppression)：是一种去除非极大值的算法，常用于计算机视觉中的边缘检测、物体识别等。


#### 9.8.2 Fast R-CNN
R-CNN的性能瓶颈在于对每个提议区域独立抽取特征。由于区域通常有大量重叠，独立的特征抽取会导致大量的重复计算。
Fast R-CNN 的一个主要改进在于只对整个图像做卷积神经网络的前向计算。

#### 9.8.3 Faster R-CNN
Fast R-CNN通常需要在选择性搜索中生成较多的提议区域，以获得较精确的目标检测结果。Faster R-CNN提出将选择性搜索替换成区域提议网络（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。
#### 9.8.4 Mask R-CNN
如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利用这些详尽的标注信息进一步提升目标检测的精度。
Mask R-CNN在Faster R-CNN的基础上做了修改。Mask R-CNN将兴趣区域池化层替换成了兴趣区域对齐层，即通过双线性插值（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。兴趣区域对齐层的输出包含了所有兴趣区域的形状相同的特征图。它们既用来预测兴趣区域的类别和边界框，又通过额外的全卷积网络预测目标的像素级位置。我们将在9.10节（全卷积网络）介绍如何使用全卷积网络预测图像中像素级的语义。
### 9.9 语义分割和数据集


# Semantic Segmentation 语义分割
语义分割需要在像素级别来判断类别，进行精确分割。
CNN的问题：CNN在进行卷积和池化过程中丢失了图像细节。feature map size逐渐变小，丢失部分轮廓信息、不能在像素级别进行分类，无法做到精确分割。另外，全连接层经过softmax后，可以获得概率信息。这些概率信息是一维的，不能标识每个像素点的类别。

## FCN
全卷积⽹络（fully convolutional network，FCN）采⽤卷积神经⽹络实现了从图像像素到像素类别的变换。
FCN是语义分割的基本框架，后续算法基于这个框架改进而来。FCN可以实现影像的密级逐像素分类。

CNN网络在卷积层之后会连接若干个全连接层, 卷积层产生的特征图(feature map)会被映射成固定长度的特征向量。
经典CNN结构(e.g. AlexNet)适合于图像层面的分类和回归任务，因为它们最后都期望得到整个输入图像的一个数值描述（概率），比如AlexNet的ImageNet模型输出一个1000维的向量表示输入图像属于每一类的概率(softmax归一化)。

FCN的输入图像可为任意尺寸，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。

最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。下图是Longjon用于语义分割所采用的全卷积网络(FCN)的结构示意图：

#### 转置卷积层
参见卷积运算 -> 转置卷积
### 全卷积
### 上采样（Upsampling）
恢复图像尺寸。上采样可以对每个像素产生一个预测，也能还原像素在原图中的位置。
### 跳跃结构
![](.\img\1.png)
对原图进行 $5$ 组卷积和池化操作，尺寸缩小为输入大小的 $1/32$，再经过两个 $1\times1$ 卷积，尺寸不变的基础熵，~~图像的维度发生变化~~???，对输出结果 `conv7` 的特征图进行 $32$ 倍的上采样得到原图大小的预测图 (1) `FCN-32`。该预测图比较粗糙，仅展示了空间的大致框架，缺少细节信息。
另使用_特征图融合_：将 `conv7` 的特征图经过 2 倍的上采样得到原图 $1/16$ 的尺寸，再与 `pool4` 的特征图对应元素相加后，得到 $B$ ，再对 $B$ 进行 16 倍的上采样得到预测图 (2) `FCN-16s`。

同理，分别对 `conv7` 进行 4 倍、`pool4` 进行 2 倍的上采样后相加[即对 $B$ 进行 2 倍上采样]，再与 `pool3` 的特征图对应元素相加，进行 $8$ 倍的上采样，得到预测图 (3) `FCN-8s`。

实验结果表明，(3) `FCN-8s` 的预测结果最优，(2) `FCN-16s` 次之，(1) `FCN-32s` 最差。
因为 (3) `FCN-8` 充分利用了 `pool3`、`pool4` 和 `conv7` 的特征图，结合了三者的细节信息和语义信息，因此结果最好。所以，跳跃结构能够优化输出结果，一定程度上实现识别与定位精度的并存。

## U-Net
包括 它由左半边的**压缩通道**（Contracting Path）和右半边**扩展通道**（Expansive Path）组成。

特点：
(1) U-Net 模型是一个编码-解码的结构，**压缩通道**编码，逐层提取特征，**扩展通道**解码，还原位置信息，且 U-Net 模型的每一个隐藏层都有较多的特征维数，这有利于模型学习更加多样、全面的特征。

 (2) U-Net 模型的 “U形” 结构让裁剪和拼接过程更加直观、合理，高层特征图与低层特征图的拼接以及卷积的反复、连续操作，使得模型能够从上下文信息和细节信息中组合得到更加精确的输出特征图。实验证明，U-Net 模型在较少的训练样本情况下也能得到更加准确的分类结果。

**压缩通道**是卷积神经网络结构，它重复包含 2 个卷积层和 1 个最大池化层的结构，每进行一次后特征图维数就增加 1 倍。
**扩展通道**会重复[1]进行反卷积操作 (特征图的维数减半)，[2]拼接同层对侧，从**压缩通道**得到的特征图裁剪出，重新组成一个 2 倍大小的特征图<sup>Overlap-tile</sup>，再经过 2 个卷积层进行特征提取，不断重复。在最后的输出层，通过两次卷积层，将 64 维的特征图映射为 2 维的输出图。
 
### 数据增强
包括 色彩抖动 尺度变换 翻转变换 平移变换 噪声扰动 _随机弹性变形_

### Overlap-tile 策略[^yinyong1]
Overlap-tile 策略使得任意大小的输入图像都可以获得一个无缝分割，因为输出的分割图它包含的像素点，它们的周围像素点（上下文）都出现在了输入图像中，因此使用Overlap-tile策略对数据进行预处理是有必要的。

上图是针对任意大小的输入图像的无缝分割的 Overlap-tile 策略。如果我们要预测黄色框内区域（即对黄色的内的细胞进行分割，获取它们的边缘），需要将蓝色框内部分作为输入，如果黄色区域在输入图像的边缘的话，那么缺失的数据使用镜像进行补充。如上图左边图像所示，输入图像周围一圈都进行了镜像补充。

因为进行的是 valid卷积[^valid]，即上下文只取有效部分，可以理解为padding为0，卷积之后的图像尺寸会改变，所以需要取比黄色框大的图像来保证上下文的信息是有意义的，缺失的部分用镜像的方法补充是填充上下文信息最好的方法了。这种方法通常需要将图像进行分块的时候才使用。

那么为什么要对图像分块，不输入整张图像呢？因为内存限制，有的机器内存比较小，需要分块输入。但比之前的滑窗取块要好很多，一方面不用取那么多块，另一方面块之间也没有那么大的区域重叠。通过Overlap-tile 策略可以将图像分块输入，否则的话就只能对图像进行 resize 了，但是这样会降低输入图像的分辨率。

此外，如果数据不够的话可以应用弹性形变，对数据进行增强，增加数据量，这允许网络可以学习到这种形变的不变性，并且并不要求带有原始预料库进行到这样的变化（指弹性形变）。

[^yinyong1]: https://zhuanlan.zhihu.com/p/46251798
[^valid]: 卷积的三种模式之一

## 超参数和可调整参数
超参数指*需要预先设定*的初始化参数，如网络层数、激活函数、卷积核大小等；
可调整参数指在模型*训练过程中被不断调整*的参数，主要指隐层的权重和偏置项等。这直接决定了模型输出结果的精度。

因此深度神经网络模型训练的目的是为了得到最佳的模型参数组合。
最常用的模型训练方法是反向传播算法（Back Propagation），又称 BP 算法。

### 损失函数
$$p_{k}(\mathbf{x})=\exp \left(a_{k}(\mathbf{x})\right) /\left(\sum_{k^{\prime}=1}^{K} \exp \left(a_{k^{\prime}}(\mathbf{x})\right)\right)$$

像素点形式的 softmax, $a_k(x)$ 表示像素 $x$ 在特征图中的第 $k$ 层的激活值，$k$ 表示是第几个特征通道，$x$ 表示像素点，$K$ 表示类別的个数。

